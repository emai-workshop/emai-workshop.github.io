
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<!--   <meta name="description" content="EMAI"> -->
<!--   <meta name="keywords" content="EMAI"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ü§ñ EMAI</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EMAI<br />
              <p class="title is-4 publication-title"> <p class="title is-3 publication-title">EMbodied AI: Trends, Challenges, and Opportunities (EMAI 2024)
              </p>
              
            </h1>
            <h1 class="is-is-5" style="color: #5c5c5c;">in conjunction with ICIP 2024, Abu Dhabi, UAE.</h1>
            <b>Time - TBD</b>
            <br>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#overview" class="button">Overview</a>
          <a href="#schedule" class="button">Schedule</a>
          <a href="#speakers" class="button">Speakers</a>
          <a href="#dates" class="button">Dates</a>
          <!-- <a href="#accepted" class="button">Accepted Papers</a> -->
          <!-- <a href="#challenge" class="button">Challenge</a> -->
          <a href="#organizers" class="button">Organizers</a>
          <div class="dropdown">
            <span class="button"><i class="fa fa-bars"></i></span>
            <div class="dropdown-content">
              <a href="https://2024.ieeeicip.org/">ICIP 2024</a>
              <!-- <a href="index_iccv23.html">ICCV 2023</a> -->
            </div>
        </h2>
        <!-- <h3 class="subtitle has-text-centered">

        </h3> -->
      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title" id="overview">Overview üí°</h2>
          <div class="content has-text-justified">
            <p>The "Embodied AI: Exploring Trends, Challenges, and Opportunities" workshop at ICIP 2024 in Abu Dhabi, UAE, is an expansive forum dedicated to the intersection of Embodied AI and fields such as computer vision, language processing, graphics, and robotics. This workshop is designed to deepen the understanding of AI agents' capabilities in perceiving, interacting, and reasoning within their environments, thereby fostering an interdisciplinary dialogue among leading researchers and practitioners. Attendees can expect a comprehensive agenda including insightful invited talks from eminent figures in the field, a poster session showcasing cutting-edge research, and engaging panel discussions aimed at debating the future directions of intelligent, interactive systems. This event promises to be a pivotal gathering for those keen to contribute to and shape the ongoing advancements in Embodied AI.
            </p>
            <P>The dedicated workshop on Embodied AI is essential due to its unique focus on integrating physical embodiment with AI capabilities, addressing challenges and opportunities not fully explored in the main ICIP conference. It merges computer vision and robotics, pushing beyond traditional boundaries to create agents that perceive, interact, and reason within their environments. This specialized forum encourages cross-disciplinary collaboration, fostering advancements that are vital for the development of intelligent, interactive systems, and addressing the gap between current image processing techniques and the future needs of AI research, including foundation models, robotics, and embodied intelligence.
            </P>
            <P>The EMAI 2024 workshop stands at the confluence of Embodied AI and pivotal areas such as computer vision, natural language processing, graphics, and robotics. This synthesis is poised to catalyze significant momentum in the field, by bringing the frontier of foundation models, robotics, and embodied AI to the research community.
            </P>
          </div>
        </div>
      </section>

      <!-- <section class="section" id="News">
        <div class="container is-max-desktop content">
          <h2 class="title">News üì∞</h2>
          <div class="content has-text-justified">
            <ul>
              <li><b>April 2023</b>: Workshop website is online.</li>
            </ul>
          </div>
        </div>
      </section> -->
      <section class="section" style="margin-top: -50px">
        <div class="container is-max-desktop">
          <section class="section" id="Schedule">
            <div class="container is-max-desktop content">
              <h2 class="title" id="schedule">Schedule ‚è∞ (tentative)</h2>
              <div class="content has-text-justified">
                <table class="table table-striped">
                  <tr>
                    <td width="200">9:00 - 9:10 AM</td>
                    <td width="600" style="background-color:#e4ffc2">Opening Remarks: An Introduction of Embodied AI</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>9:10 - 10:00 AM</td>
                    <td style="background-color:#cae1ff">Keynote - Embodied AI with World Models (TBD)<b></b></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>10:00 - 10:45 AM</td>
                    <td style="background-color:#ffe0c6">Oral Session for Accepted Papers</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>10:45 - 11:00 AM</td>
                    <td style="background-color:#eeeeee">Coffee Break</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>11:00 - 11:30 AM</td>
                    <td style="background-color:#cae1ff">Invited Talk - Embodied Visual Navigation in Complex Environments (TBD)<b></b></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>11:30 - 12:00 NOON</td>
                    <td style="background-color:#ffe0c6">Oral Session for Accepted Papers</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>12:00 NOON - 2:00 PM</td>
                    <td style="background-color:#ffffff">Poster Session for Accepted Papers & Lunch Time</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2:00 - 2:45 PM</td>
                    <td style="background-color:#cae1ff">Keynote - When Foundation Models Meets Embodied AI (TBD)</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>2:45 - 3:45 PM</td>
                    <td style="background-color:#ffe0c6">Oral Session for Accepted Papers</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>3:45 - 4:00 PM</td>
                    <td style="background-color:#eeeeee">Coffee Break</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>4:00 - 4:30 PM</td>
                    <td style="background-color:#cae1ff">Invited Talk - Sim to Real: Bridging Virtual and Physical Worlds (TBD)<b></b></td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>4:30 - 5:00 PM</td>
                    <td style="background-color:#ffe0c6">Oral Session for Accepted Papers</td>
                    <td></td>
                  </tr>
                </table>
              </div>
            </div>
          </section>

          <section class="section" id="Invited Speakers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="speakers">Invited Speakers üßë‚Äçüè´</h2>

              <a href="" target="_blank">
                <div class="card">
                  <div class="card-content">
                    <div class="columns is-vcentered">
                      <div class="column is-one-quarter">
                        <figure class="image is-128x128">
                          <img class="is-rounded" src="static/img/kristen.jpg">
                        </figure>
                      </div>
                      <div class="column">
                        <p class="title is-4">TBD</p>
                        <p class="subtitle is-6">TBD</p>
                      </div>
                    </div>
                    <div class="content">
                      <!-- Kristen Grauman is a Professor in the Department of Computer Science at the University of Texas at
                      Austin and a Research Director in Facebook AI Research (FAIR).
                      Her research in computer vision and machine learning focuses on video, visual recognition, and
                      action for perception or embodied AI.
                      Before joining UT-Austin in 2007, she received her Ph.D. at MIT.
                      She is an IEEE Fellow, AAAI Fellow, Sloan Fellow, a Microsoft Research New Faculty Fellow, and a
                      recipient of NSF CAREER and ONR Young Investigator awards,
                      the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the
                      International Joint Conference on Artificial Intelligence (IJCAI),
                      the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013.
                      She was inducted into the UT Academy of Distinguished Teachers in 2017.
                      She and her collaborators have been recognized with several Best Paper awards in computer vision,
                      including a 2011 Marr Prize and a 2017 Helmholtz Prize (test of time award). -->
                    </div>
                  </div>
                </div>
              </a>

              <a href="" target="_blank">
                <div class="card">
                  <div class="card-content">
                    <div class="columns is-vcentered">
                      <div class="column is-one-quarter">
                        <figure class="image is-128x128">
                          <img class="is-rounded" src="static/img/jiajun.jpg">
                        </figure>
                      </div>
                      <div class="column">
                        <p class="title is-4">TBD</p>
                        <p class="subtitle is-6">TBD</p>
                      </div>
                    </div>
                    <div class="content">
                      <!-- Jiajun Wu is an Assistant Professor of Computer Science at Stanford University, working on
                      computer vision, machine learning, and computational cognitive science.
                      Before joining Stanford, he was a Visiting Faculty Researcher at Google Research.
                      He received his PhD in Electrical Engineering and Computer Science from the Massachusetts
                      Institute of Technology.
                      Wu's research has been recognized through the Young Investigator Programs (YIP) by ONR and by
                      AFOSR, paper awards and finalists at ICCV, CVPR, SIGGRAPH Asia, CoRL, and IROS, dissertation
                      awards from ACM, AAAI, and MIT,
                      the 2020 Samsung AI Researcher of the Year, and faculty research awards from J.P. Morgan, Samsung,
                      Amazon, and Meta. -->
                    </div>
                  </div>
                </div>
              </a>
              <a href="" target="_blank">
                <div class="card">
                  <div class="card-content">
                    <div class="columns is-vcentered">
                      <div class="column is-one-quarter">
                        <figure class="image is-128x128">
                          <img class="is-rounded" src="static/img/cmk.jpg">
                        </figure>
                      </div>
                      <div class="column">
                        <p class="title is-4">TBD</p>
                        <p class="subtitle is-6">TBD</p>
                      </div>
                    </div>
                    <div class="content">
                      <!-- Chung Min Kim is a PhD student at UC Berkeley,
                      where she is advised by Ken Goldberg and Angjoo Kanazawa.
                      She received her dual B.S. degree in EECS (Electrical Engineering and Computer Science) and
                      Mechanical Engineering from UC Berkeley in 2021.
                      She is currently funded by the NSF GRFP.
                      Her research interests include 3D scene understanding for computer vision and robotics.
                      In particular, she is interested in modeling multi-scale semantics with 3D, using large
                      vision-language models.
                      Her goal is to apply these models to robots in the real world, which is challenging due to lack of
                      structure and large variability in the real world. -->
                    </div>
                  </div>
                </div>
              </a>

              <a href="" target="_blank">
                <div class="card">
                  <div class="card-content">
                    <div class="columns is-vcentered">
                      <div class="column is-one-quarter">
                        <figure class="image is-128x128">
                          <img class="is-rounded" src="static/img/justin.jpg">
                        </figure>
                      </div>
                      <div class="column">
                        <p class="title is-4">TBD</p>
                        <p class="subtitle is-6">TBD</p>
                      </div>
                    </div>
                    <div class="content">
                      <!-- Justin Kerr is a PhD student at UC Berkeley co-advised by Ken Goldberg and Angjoo Kanazawa working
                      primarily on NeRF for robot manipulation, 3D scene understanding, and visuo-tactile representation
                      learning.
                      Recently Justin is interested in leveraging NeRF for language grounding, and how it could change
                      how we interact with 3D.
                      His work is supported by the NSF GRFP. Previously he finished my bachelor's at CMU where he worked
                      with Howie Choset on multi-agent path planning, and spent time at Berkshire Grey and NASA's JPL. -->
                    </div>
                  </div>
                </div>
              </a>
            <br />
            </div>
          </section>

          <section class="section" id="Related Works">
            <!-- <div class="container is-max-desktop content">
              <h2 class="title" id="relatedwork">Related Works üßë‚Äçü§ù</h2>
              Below is a collection of concurrent and related works in the field of open-set 3D scene understanding.
              Please
              feel free to get in touch to add other works as well.
              <ul>
                <li><a href="https://concept-fusion.github.io/">ConceptFusion: Open-set Multimodal 3D Mapping</a> RSS'23
                </li>
                <li><a href="https://pengsongyou.github.io/openscene">OpenScene: 3D Scene Understanding with Open
                    Vocabularies</a> CVPR'23</li>
                <li><a href="https://www.lerf.io">LERF: Language Embedded Radiance Fields</a> ICCV'23</li>
                <li><a href="https://pfnet-research.github.io/distilled-feature-fields/">Decomposing NeRF for Editing
                    via Feature Field Distillation</a> NeurIPS'22</li>
                <li><a href="https://semantic-abstraction.cs.columbia.edu/">Semantic Abstraction: Open-World 3D Scene
                    Understanding from 2D Vision-Language Models</a> CoRL'22</li>
                <li><a href="https://rozdavid.github.io/scannet200">Language-Grounded Indoor 3D Semantic Segmentation in
                    the Wild</a> ECCV'22</li>
                <li><a href="https://3dlg-hcvc.github.io/multiscan/">MultiScan: Scalable RGBD Scanning for 3D
                    Environments with Articulated Objects</a> NeurIPS'22</li>
                <li><a href="https://github.com/NVlabs/ODISE">ODISE: Open-Vocabulary Panoptic Segmentation with
                    Text-to-Image Diffusion Models</a> CVPR'23</li>
                <li><a href="https://github.com/Kunhao-Liu/3D-OVS">Weakly Supervised 3D Open-Vocabulary Segmentation</a>
                  NeurIPS'23</li>
                <li><a href="https://arxiv.org/abs/2306.02329">Multi-CLIP: Contrastive Vision-Language Pre-training for
                    Question Answering tasks in 3D Scenes</a></li>
                <li><a href="https://openmask3d.github.io">OpenMask3D: Open-Vocabulary 3D Instance Segmentation</a>
                  NeurIPS'23</li>
                <li><a href="https://arxiv.org/abs/2303.04748">CLIP-FO3D: Learning Free Open-world 3D Scene
                    Representations from 2D Dense CLIP</li>
                <li><a href="https://tsagkas.github.io/vl-fields/">VL-Fields: Towards Language-Grounded Neural Implicit
                    Spatial Representations</a> ICRA'23</li>
                <li><a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">PLA:
                    Language-Driven Open-Vocabulary 3D Scene Understanding</a> CVPR'23</li>
                <li><a href="https://arxiv.org/abs/2304.00962">RegionPLC: Regional Point-Language Contrastive Learning
                    for Open-World 3D Scene Understanding</a></li>
                <li><a href="https://arxiv.org/abs/2309.00616">OpenIns3D: Snap and Lookup for 3D open-vocabulary
                    Instance Segmentation</a></li>
                <li><a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf">ConceptGraphs:
                    Open-Vocabulary 3D Scene Graphs for Perception and Planning</a></li>
                <li><a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Open-Vocabulary
                    Point-Cloud Object Detection without 3D Annotation</a> CVPR'23</li>
                <li><a href="https://github.com/yangcaoai/CoDA_NeurIPS2023">CoDA: Collaborative Novel
                    Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</a> NeurIPS'23
                </li>
                <br>
              </ul>
              and many more ...
              </ul>
          </div -->
          </section>
          
          <section class="section" style="margin-top: -50px">
            <div class="container is-max-desktop">
              <section class="section" id="Motivation">
                <div class="container is-max-desktop content">
                  <h2 class="title" id="overview">Call for Papers üìù</h2>
                  <div class="content has-text-justified">
                  <p>We warmly invite submissions of high-quality research papers, not exceeding 4 pages (excluding references), that focus on the following themes of Embodied AI:</p>
                    <li >Foundation Models in Embodied AI</li>
                    <li>World Models in Embodied Intelligence</li> 
                    <li>Generative Model in Embodied AI </li>
                    <li>Visual Question Answering in Embodied AI </li> 
                    <li>Embodied AI for Visual Navigation                 </li> 
                    <li>Embodied AI for Manipulation & Planning        </li> 
                    <li>Embodied AI for Multimodal Processing  </li>     
                    <li>Embodied AI-Human Interaction</li> 
                    
                  <br>
                  <p>Selected papers will earn the opportunity for presentation in the form of either posters or spotlight talks during the workshop. Additionally, these papers will be published and made accessible via IEEE Xplore, adhering to the <a href ="https://cmsworkshops.com/ICIP2024/papers/paper_kit.php"> ICIP's guidelines </a> for workshop contributions. Please note, as per ICIP regulations, at least one author from each accepted paper is required to complete an in-person registration for the conference.</p>
                  <p>The submission deadline is April 25, 2024 (Anywhere on Earth). Papers should be no longer than 4 pages (excluding references) and styled in the ICIP <a href = "https://cmsworkshops.com/ICIP2024/papers/paper_kit.php">format</a>.</p>
                  <p>Submission Link : <a href = "https://urldefense.proofpoint.com/v2/url?u=https-3A__cmsworkshops.com_ICIP2024_papers_submission.asp-3FType-3DWS-26ID-3D4&d=DwIFaQ&c=slrrB7dE8n7gBJbeO0g-IQ&r=Nr6S6uI8GLu6TmJ7zqgFrA&m=iqxFpl5jtpkohRzLo3ZS-o6rPlDgYuz09yQjGisvyexypnagV8jAsC7RYz4Dra35&s=GMettIFex-NL1WZjDE2ItlJS2JG63DOkzBeyykhwiE0&e="><b>Link</b></a></p>
                  </div>
                </div>
              </section>         
          
          <section class="section" id="Papers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="dates">Important Dates üóìÔ∏è</h2>
              <ul>
                <!-- <li><b>Paper Submission Deadline</b> -->
                  <!-- : We accept novel full 8-page papers for publication in the proceedings, and
                  either shorter
                  4-page extended abstracts or 8-page papers of novel or previously published work that will not
                  be included in
                  the
                  proceedings. All submissions have to follow the <a
                    href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines">CVPR 2024 author
                    guidelines.</a>
                </li>
                <ul>
                  <li><b>Submission Portal</b>: <a href="https://cmt3.research.microsoft.com/OpenSUN3D2024">CMT</a></li> -->
                  <li><b>Paper Submission Deadline</b>: April 25, 2024.</li>
                  <li><b>Paper Acceptance Notification</b>: June 6, 2024</li>
                  <li><b>Final Paper Submission Deadlin</b>: June 19, 2024</li>
                  <li><b>Author Registration Deadline</b>: July 11, 2024</li>
                </ul>
                <!-- <li><b>Challenge Track 1</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_1">Open-vocabulary 3D object instance
                    search</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li>

                </ul>
                <li><b>Challenge Track 2</b>: <a
                    href="https://opensun3d.github.io/cvpr24-challenge/track_2">Open-vocabulary 3D affordance
                    grounding</a></li>
                <ul>
                  <li><b>Submission Portal</b>: EvalAI</li>
                  <li><b>Data Instructions & Helper Scripts</b>: April 15, 2024</li>
                  <li><b>Dev Phase Start</b>: April 15, 2024</li>
                  <li><b>Submission Portal Start</b>: April 15, 2024</li>
                  <li><b>Test Phase Start</b>: May 1, 2024</li>
                  <li><b>Test Phase End</b>: June 8, 2024 (23:59 Pacific Time)</li> -->

                </ul>
              </ul>
            </div>
          </section>

          <!-- <section class="section" id="Challenge">
            <div class="container is-max-desktop content">
              <h2 class="title" id="challenge">Challenge</h2>
              (Please check <a href="https://opensun3d.github.io/index_iccv23.html#challengeresults">this page</a> out
              for an overview of last year's challenge results.
              We have also published a <a href="https://arxiv.org/abs/2402.15321"> technical report</a> providing an
              overview of our ICCV 2023 workshop challenge.)
              <br>
              <br>
            </div>
            This year, our challenge will consist of two tracks:

            <li>Track 1: Open-vocabulary 3D object instance search</li>
            <li>Track 2: Open-vocabulary 3D affordance grounding</li>

            <br>
          </section> -->


          <section class="section" id="Organizers">
            <div class="container is-max-desktop content">
              <h2 class="title" id="organizers">Organizers</h2>

              <div class="columns is-centered is-variable is-0">
                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/fang.jpg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Yi Fang</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/hao.jpeg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Hao Huang</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/yuan.jpeg"
                            alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Shuaihang Yuan</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>

                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/liu.jpeg" alt="Placeholder image"> 
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Yu-Shen Liu</p>
                            <p class="subtitle is-7">Tsinghua University, Beijing, China</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>

              </div>
              <div class="columns is-centered is-variable is-0">

                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/tuka.jpg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Tuka Waddah Alhanai</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>


                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/yu.png" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Yu Hao</p>
                            <p class="subtitle is-7">New York University Abu Dhabi, UAE</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/jen.jpeg" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Junsheng Zhou</p>
                            <p class="subtitle is-7">Tsinghua University, Beijing, China</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
                <div class="column is-one-quarter">
                  <a href="">
                    <div class="card">
                      <div class="card-image">
                        <figure class="image">
                          <img class="is-rounded" src="./static/images/geeta.png" alt="Placeholder image">
                        </figure>
                      </div>
                      <div class="card-content">
                        <div class="media">
                          <div class="media-content" style="overflow-x: unset;">
                            <p class="title is-7 is-spaced">Geeta Chandra Raju, Bethala</p>
                            <p class="subtitle is-7">New York University, New York, USA</p>
                          </div>
                        </div>
                      </div>
                    </div>
                  </a>
                </div>
              </div>
            </div>
          </section>

          <!-- <section class="section" id="Sponsors">
        <div class="container is-max-desktop content">
          <h2 class="title">Sponsors </h2>
        </div>
      </section> -->

          <footer class="footer">
            <div class="container">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  <br />
                  It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
                  We would like to thank Utkarsh Sinha and Keunhong Park.
                </p>
              </div>
            </div>
          </footer>
</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
